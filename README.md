# SWIFT MT564 AI
# ğŸš€ SWIFT-MT564-AI: Anomaly Detection in Corporate Action Messages

This project focuses on detecting **structural and compliance anomalies** in SWIFT MT564 corporate action messages using a **fine-tuned Google Gemma 3B model** with LoRA (Low-Rank Adaptation).

---

## ğŸ’¡ Objective

To build an AI assistant that:
- Parses SWIFT MT564 message structure
- Flags field-level and sequence-level anomalies
- Provides actionable insights for compliance and reconciliation teams

---

## ğŸ” Model Details

- **Base Model**: `google/gemma-3-1b-it`
- **Fine-Tuned Using**: LoRA on `transformers + peft`
- **Training Format**: Instruction-tuned JSONL
- **Sample Prompt Format**:

Train TinyLlama or Gemma models to detect anomalies in SWIFT MT564 messages.

Instruction:
Analyze this MT564 message for anomalies

Input:
{1:F01BANKXXXX...}{2:I564BANKYYYY...}{4:
:16R:GENL
:20C::CORP//CA20250501
...
}

Response:
Missing Sequence B

Unusual currency

Sanctioned recipient

yaml
Copy
Edit

---

## ğŸ“ Directory Structure

SWIFT-MT564-AI/
â”‚
â”œâ”€â”€ app.py # Inference interface (API or CLI)
â”œâ”€â”€ main.py # Entry point
â”œâ”€â”€ prepare_mt564_data.py # Training data generator
â”œâ”€â”€ prepare_data.py # Generic data preprocessing
â”œâ”€â”€ train_mt564_model.py # Trainer for baseline models
â”œâ”€â”€ train_tinyllama.py # Legacy: TinyLlama training
â”œâ”€â”€ requirements.txt # All dependencies
â”œâ”€â”€ mt564-gemma-lora/ # Fine-tuned model dir
â”œâ”€â”€ data/ # Raw training data (SWIFT JSON)
â”œâ”€â”€ README.md # Project readme
â”œâ”€â”€ .gitignore # Git exclusions

yaml
Copy
Edit

---

## ğŸ§ª Local Inference

```bash
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("pareshmishra/mt564-gemma-lora")
model = AutoModelForCausalLM.from_pretrained("pareshmishra/mt564-gemma-lora")

input_text = "### Instruction:\nAnalyze this MT564 message for anomalies\n\n### Input:\n{1:F01...}"
inputs = tokenizer(input_text, return_tensors="pt")

output = model.generate(**inputs, max_new_tokens=512)
print(tokenizer.decode(output[0], skip_special_tokens=True))
ğŸ”„ Training (LoRA)
See train_mt564_model.py or use Colab/AutoTrain with the formatted JSONL file.

ğŸ§  Model Hosted at
ğŸ‘‰ Hugging Face Model Page

ğŸ“š Resources Used
SWIFT ISO20022 documentation

Custom JSONL training entries

Google Gemma 3B IT Model

Hugging Face transformers, peft, datasets

ğŸ“¬ Contact
For questions or contributions: @pareshmishra

MIT License. For research or educational use only.
 






 

